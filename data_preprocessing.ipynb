{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Milestone 2 - CasierVert952"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a part of the [analysis.ipynb](analysis.ipynb) that performs the preprocessing of the data from two beers ratings websites (BeerAdvocate and RateBeer).\n",
    "\n",
    "## Preprocessing\n",
    "\n",
    "### 1 Importation of libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the basic requiered libraries\n",
    "import os\n",
    "import csv\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Path variables\n",
    "BA_DATA_PATH = \"data/BeerAdvocate/\"\n",
    "RB_DATA_PATH = \"data/RateBeer/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 Transformation of TXT ratings files to CSV\n",
    "\n",
    "You can download the ```ratings.csv``` files for both dataset with the following links (~2GB each):\n",
    "\n",
    "- For BA : [here](https://coursedingler.ch/data/BA/ratings.csv)\n",
    "- For RB : [here](https://coursedingler.ch/data/RB/ratings.csv)\n",
    "\n",
    "The following cell should **NOT** be executed, it only shows how the ```ratings.csv``` for each dataset were generated.\n",
    "\n",
    "It take around 19 minutes to generate the BA ratings file and 14 minutes for the RB one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "from helpers import txt_to_csv\n",
    "\n",
    "file_txt = 'ratings.txt'\n",
    "file_csv = 'ratings.csv'\n",
    "\n",
    "txt_to_csv(BA_DATA_PATH + file_txt, BA_data_path + file_csv, \"BA\")\n",
    "txt_to_csv(RB_DATA_PATH + file_txt, RB_data_path + file_csv, \"RB\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Make sure you have been placed or generated the ```ratings.csv``` files in the ```BeerAdvocate``` and ```RateBeer``` folders as well as all other data files when executing the following cells !**\n",
    "```\n",
    "data/\n",
    "├── BeerAdvocate\n",
    "│   ├── beers.csv\n",
    "│   ├── breweries.csv\n",
    "│   ├── users.csv\n",
    "│   └── ratings.csv\n",
    "│\n",
    "└── RateBeer\n",
    "    ├── beers.csv\n",
    "    ├── breweries.csv\n",
    "    ├── users.csv\n",
    "    └── ratings.csv\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### 3 Loading CSV data\n",
    "Let first import the data in CSV format for the two datasets, the users, the beers and the breweries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dataframes for the BA dataset\n",
    "BA_beers = pd.read_csv(BA_DATA_PATH + 'beers.csv')\n",
    "BA_breweries = pd.read_csv(BA_DATA_PATH + 'breweries.csv')\n",
    "BA_users = pd.read_csv(BA_DATA_PATH + 'users.csv')\n",
    "\n",
    "# Create Dataframes for the RB dataset\n",
    "RB_beers = pd.read_csv(RB_DATA_PATH + 'beers.csv')\n",
    "RB_breweries = pd.read_csv(RB_DATA_PATH + 'breweries.csv')\n",
    "RB_users = pd.read_csv(RB_DATA_PATH + 'users.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/BeerAdvocate/ratings.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Create Dataframes for the BA ratings file\u001b[39;00m\n\u001b[1;32m      2\u001b[0m s_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m----> 3\u001b[0m BA_rating \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mBA_DATA_PATH\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mratings.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m e_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReading of BA ratings ended in \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(e_time \u001b[38;5;241m-\u001b[39m s_time) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m seconds.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m   1014\u001b[0m     dialect,\n\u001b[1;32m   1015\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m   1023\u001b[0m )\n\u001b[1;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pandas/io/parsers/readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1887\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1889\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pandas/io/common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    874\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    875\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    876\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    877\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/BeerAdvocate/ratings.csv'"
     ]
    }
   ],
   "source": [
    "# Create Dataframes for the BA ratings file\n",
    "s_time = time.time()\n",
    "BA_rating = pd.read_csv(BA_DATA_PATH + 'ratings.csv')\n",
    "e_time = time.time()\n",
    "print(\"Reading of BA ratings ended in \" + str(e_time - s_time) + \" seconds.\")\n",
    "\n",
    "# Create Dataframes for the RB ratings file\n",
    "s_time = time.time()\n",
    "RB_rating = pd.read_csv(RB_DATA_PATH + 'ratings.csv')\n",
    "e_time = time.time()\n",
    "print(\"Reading of RB ratings ended in \" + str(e_time - s_time) + \" seconds.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4 Merging data\n",
    "\n",
    "#### 4.1 Dropping and renaming columns\n",
    "\n",
    "We are dropping the columns that are not needed, some of them will be recovered during the merging phase. The columns are renamed to avoid colisions during the merges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from helpers import ratings_dict\n",
    "\n",
    "# Removing not needed columns\n",
    "BA_rating.drop(columns=[\"text\", \"review\"], inplace=True)\n",
    "RB_rating.drop(columns=[\"text\"], inplace=True)\n",
    "\n",
    "# Removing columns that will be recovered when merging\n",
    "BA_rating.drop(columns=[\"brewery_name\", \"style\", \"beer_name\", \"user_name\", \"abv\"], inplace=True)\n",
    "RB_rating.drop(columns=[\"brewery_name\", \"style\", \"beer_name\", \"user_name\", \"abv\"], inplace=True)\n",
    "\n",
    "# Renaming the columns as define by \"ratings_dict\"\n",
    "BA_rating.rename(columns=ratings_dict, inplace=True)\n",
    "RB_rating.rename(columns=ratings_dict, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 Merging with beers 's Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers import beers_dict\n",
    "\n",
    "# Merging with the beers's data\n",
    "BA_merged = pd.merge(BA_rating, BA_beers, on=[\"beer_id\", \"brewery_id\"], how=\"inner\")\n",
    "RB_merged = pd.merge(RB_rating, RB_beers, on=[\"beer_id\", \"brewery_id\"], how=\"inner\")\n",
    "\n",
    "# Renaming the columns as define by \"beers_dict\"\n",
    "BA_merged.rename(columns=beers_dict, inplace=True)\n",
    "RB_merged.rename(columns=beers_dict, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3 Merging with breweries's data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers import breweries_dict\n",
    "\n",
    "# Merging with the breweries's data\n",
    "BA_merged = pd.merge(BA_merged, BA_breweries, left_on=\"brewery_id\", right_on=\"id\", how=\"inner\")\n",
    "RB_merged = pd.merge(RB_merged, RB_breweries, left_on=\"brewery_id\", right_on=\"id\", how=\"inner\")\n",
    "\n",
    "# Dropping the duplicate columns\n",
    "BA_merged.drop(columns=[\"id\", \"name\"], inplace=True)\n",
    "RB_merged.drop(columns=[\"id\", \"name\"], inplace=True)\n",
    "\n",
    "# Renaming the columns as define by \"breweries_dict\"\n",
    "BA_merged.rename(columns=breweries_dict, inplace=True)\n",
    "RB_merged.rename(columns=breweries_dict, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.4 Merging with users's data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers import users_dict\n",
    "\n",
    "# Merging with the users's data\n",
    "BA_merged = pd.merge(BA_merged, BA_users, on=[\"user_id\"], how=\"inner\")\n",
    "RB_merged = pd.merge(RB_merged, RB_users, on=[\"user_id\"], how=\"inner\")\n",
    "\n",
    "# Renaming the columns as define by \"users_dict\"\n",
    "BA_merged.rename(columns=users_dict, inplace=True)\n",
    "RB_merged.rename(columns=users_dict, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5 Data cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1 Changing dates format\n",
    "\n",
    "The format of the dates fields were initialy the timestamp format, here we convert it in a human readable format. We only keep the month and the year as it's the only element we need for our analyses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changing the format of the rating date to the format \"Month-Year\"\n",
    "BA_cleaned = BA_merged\n",
    "BA_cleaned[\"rating_date\"] = pd.to_datetime(BA_cleaned[\"rating_date\"], unit='s').dt.strftime(\"%m-%Y\")\n",
    "\n",
    "RB_cleaned = RB_merged\n",
    "RB_cleaned[\"rating_date\"] = pd.to_datetime(RB_cleaned[\"rating_date\"], unit='s').dt.strftime(\"%m-%Y\")\n",
    "\n",
    "# Changing the format of the user_join date to the format \"Month-Year\"\n",
    "BA_cleaned = BA_merged\n",
    "BA_cleaned[\"user_join_date\"] = pd.to_datetime(BA_cleaned[\"user_join_date\"], unit='s').dt.strftime(\"%m-%Y\")\n",
    "\n",
    "RB_cleaned = RB_merged\n",
    "RB_cleaned[\"user_join_date\"] = pd.to_datetime(RB_cleaned[\"user_join_date\"], unit='s').dt.strftime(\"%m-%Y\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2 Extracting the country and states\n",
    "\n",
    "Since our analyses will focus on american states, we have to divide the user location field in ```country``` and ```state```. The state is filled with ```nan``` if not present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spliting the \"user_location\" in country and state fields\n",
    "split_locations = BA_cleaned['user_location'].str.split(',', expand=True)\n",
    "BA_cleaned['user_country'] = split_locations[0].str.strip()\n",
    "BA_cleaned['user_state'] = split_locations[1].str.strip() if len(split_locations) > 1 else np.nan\n",
    "\n",
    "split_locations = RB_cleaned['user_location'].str.split(',', expand=True)\n",
    "RB_cleaned['user_country'] = split_locations[0].str.strip()\n",
    "RB_cleaned['user_state'] = split_locations[1].str.strip() if len(split_locations) > 1 else np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We divide breweries location field too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spliting the \"breweries_location\" in country and state fields\n",
    "split_locations = BA_cleaned['breweries_location'].str.split(',', expand=True)\n",
    "BA_cleaned['breweries_country'] = split_locations[0].str.strip()\n",
    "BA_cleaned['breweries_state'] = split_locations[1].str.strip() if len(split_locations) > 1 else np.nan\n",
    "\n",
    "split_locations = RB_cleaned['breweries_location'].str.split(',', expand=True)\n",
    "RB_cleaned['breweries_country'] = split_locations[0].str.strip()\n",
    "RB_cleaned['breweries_state'] = split_locations[1].str.strip() if len(split_locations) > 1 else np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now just changing the order of the columns for a better visibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "BA_new_column_order = [\n",
    "    'rating_date', 'rating_appearance',\n",
    "    'rating_aroma', 'rating_palate', 'rating_taste', 'rating_overall',\n",
    "    'rating', 'beer_id', 'beer_name', 'beer_style', 'beer_nbr_ratings',\n",
    "    'beer_nbr_reviews', 'beer_avg', 'beer_ba_score', 'beer_bros_score',\n",
    "    'beer_abv', 'beer_avg_computed', 'beer_zscore',\n",
    "    'beer_nbr_matched_valid_ratings', 'beer_avg_matched_valid_ratings', 'brewery_id',\n",
    "    'brewery_name', 'breweries_location', 'breweries_country', 'breweries_state', 'breweries_nbr_beers',\n",
    "    'user_id', 'user_nbr_ratings', 'user_nbr_reviews', 'user_name', 'user_join_date',\n",
    "    'user_location', 'user_country', 'user_state'\n",
    "]\n",
    "\n",
    "RB_new_column_order = [ \n",
    "    'rating_date', 'rating_appearance',\n",
    "    'rating_aroma', 'rating_palate', 'rating_taste', 'rating_overall',\n",
    "    'rating', 'beer_id', 'beer_name', 'beer_style', 'beer_nbr_ratings',\n",
    "    'overall_score', 'style_score', 'beer_avg', 'beer_abv',\n",
    "    'beer_avg_computed', 'beer_zscore', 'beer_nbr_matched_valid_ratings',\n",
    "    'beer_avg_matched_valid_ratings', 'brewery_id', 'brewery_name', 'breweries_location',\n",
    "    'breweries_country', 'breweries_state', 'breweries_nbr_beers', 'user_id', 'user_nbr_ratings', 'user_name',\n",
    "    'user_join_date', 'user_location', 'user_country', 'user_state'\n",
    "]\n",
    "BA_cleaned = BA_cleaned[BA_new_column_order]\n",
    "RB_cleaned = RB_cleaned[RB_new_column_order]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6 Saving cleaned data\n",
    "\n",
    "The following cell will save the data merged and cleaned in CSV file. After that step, the two dataframes can be loaded quickly without performing the preprocessing step again and again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the cleaned data in CSV files\n",
    "BA_cleaned.to_csv(BA_DATA_PATH + 'BA_cleaned.csv', index=False)\n",
    "RB_cleaned.to_csv(RB_DATA_PATH + 'RB_cleaned.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
